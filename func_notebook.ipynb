{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Necessary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create some functions below that we will be using throughout the course of the notebook. This will allow for our notebook to be much cleaner by using less code and allowing us to use these functions readily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_import(csv):\n",
    "    '''Loads the requested csv file, and provides its shape, summary information, and count of null values\n",
    "    \n",
    "        @params\n",
    "        csv is a .csv file\n",
    "        df is a pd.DataFrame\n",
    "        \n",
    "        @output\n",
    "        a pd.Dataframe\n",
    "    '''\n",
    "    df = pd.read_csv(csv)\n",
    "    print('DATA FRAME SHAPE')\n",
    "    print(\"========================================\") #this line breaks really help to separately visualize the data\n",
    "    print(df.shape)\n",
    "    print(\"\")\n",
    "    print('DATA SUMMARY')\n",
    "    print(\"========================================\")\n",
    "    print(df.info())\n",
    "    print(\"\")\n",
    "    print(\"VALUE COUNTS FOR COLUMNS WITH NULL VALUES\")\n",
    "    print(\"========================================\")\n",
    "    #create list of Null columns we are going to apend to in a for loop\n",
    "    null_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > 0: #if there are any null values above 0 in the column\n",
    "            null_cols.append(col) #append our null_cols list and add those columns\n",
    "    for col in null_cols: #now, for the values in null_cols\n",
    "        print(col, \"VALUE COUNTS\") #print the column name and the value count of null values\n",
    "        print(\"--------------------\") \n",
    "        print(df[col].value_counts(dropna=False))  #print those value counts for each column\n",
    "        print(\"\")\n",
    "    return df\n",
    "\n",
    "def describe_outliers(df):\n",
    "    '''Calls the summary function, while also identifying the value three standard deviations away from the mean\n",
    "    \n",
    "            @params\n",
    "            df is a pd.DataFrame\n",
    "    \n",
    "            @output\n",
    "            summary of a pd.DataFrame.describe() with 3 standard deviations added\n",
    "    '''\n",
    "    print('DATA DESCRIPTION')\n",
    "    print(\"========================================\")\n",
    "    describe = df.describe() #create a dateframe, using new cloumns from descriptions of df \n",
    "#Below, use loc to select create 2 new columns which will be created using standard deviation formula\n",
    "    describe.loc['+3_std'] = describe.loc['mean'] + (describe.loc['std'] * 3)\n",
    "    describe.loc['-3_std'] = describe.loc['mean'] - (describe.loc['std'] * 3)\n",
    "    print(describe)\n",
    "    \n",
    "def draw_qqplot(residual):\n",
    "    fig, ax = plt.subplots(figsize=(6,2.5))\n",
    "    _, (__, ___, r) = sp.stats.probplot(residual, plot=ax, fit=True)\n",
    "    \n",
    "    return fig, ax \n",
    "\n",
    "def draw_scatter(y_pred, residual):\n",
    "    fig, ax = plt.subplots(figsize=(6,2.5))\n",
    "    _ = ax.scatter(y_pred, residual, color='blue')\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def high_corr(df):\n",
    "    '''Produces column interactions with a p value of .70 - .99\n",
    "    \n",
    "            @params\n",
    "            df is a pd.DataFrame\n",
    "    \n",
    "            @output\n",
    "            a df of highly-correlated columns and their pearson correlation score\n",
    "    '''\n",
    "    #create a new correlated dataframe with absolute value of a number,\n",
    "    df_highcorr = df.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "    df_highcorr['Highly Correlated Pairs'] = list(zip(df_highcorr.level_0, df_highcorr.level_1))\n",
    "    df_highcorr.set_index(['Highly Correlated Pairs'], inplace = True)\n",
    "    df_highcorr.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "    df_highcorr.columns = ['Correlation']\n",
    "    df_highcorr.drop_duplicates(inplace=True)\n",
    "    return df_highcorr[(df_highcorr.Correlation>.7) & (df_highcorr.Correlation<1)]\n",
    "    \n",
    "def log_transform(df,features):\n",
    "    '''Runs a log transformation on a feature\n",
    "    \n",
    "        @params\n",
    "        df is a pd.Dataframe\n",
    "        features is a list of columns to be considered\n",
    "        \n",
    "        @output\n",
    "        new log-transformed column\n",
    "    \n",
    "    '''\n",
    "    for feature in features:\n",
    "        df[feature + '_log'] = np.log(df[feature])\n",
    "    return df\n",
    "    \n",
    "def quick_corrmap(df, features):\n",
    "    '''Quickly produces a correlation heatmap\n",
    "    \n",
    "            @params\n",
    "            df is a pd.DataFrame\n",
    "            features is a list of columns to be considered\n",
    "    \n",
    "            @output\n",
    "            correlation heat map\n",
    "    '''\n",
    "    mask = np.zeros_like(df[features].corr(), dtype=np.bool) #masking visually cuts the graph in half\n",
    "    mask[np.triu_indices_from(mask)] = True \n",
    "\n",
    "    f, ax = plt.subplots(figsize=(16, 12))\n",
    "    plt.title('Pearson Correlation Matrix',fontsize=25)\n",
    "\n",
    "    sns.heatmap(df[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"BuGn\", #\"BuGn_r\" to reverse \n",
    "            linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9});\n",
    "\n",
    "def reg_summary(X_train, y_train):\n",
    "    X_with_constant = sm.add_constant(X_train)\n",
    "    model = sm.OLS(y_train, X_with_constant)\n",
    "    results = model.fit()\n",
    "    print(results.params)\n",
    "    print(results.summary())\n",
    "    return results\n",
    "    \n",
    "def remove_outliers(df, features):\n",
    "    '''Removes outliers more than 3 standard deviations away from the mean for each listed feature\n",
    "    \n",
    "        @params\n",
    "        df is a pd.Dataframe\n",
    "        features is a list of columns to be considered\n",
    "        \n",
    "        @output\n",
    "        df with outliers removed\n",
    "    '''\n",
    "    print(\"COUNT OF OUTLIERS REMOVED\")\n",
    "    print(\"========================================\")\n",
    "    \n",
    "    x = len(df)\n",
    "    \n",
    "    for feature in features:\n",
    "        df[feature + '_zscore'] = np.abs(stats.zscore(df[feature]))\n",
    "        y = df.loc[np.abs(df[feature + '_zscore']) > 3]\n",
    "        percent = round((len(y) * 100) / x, 3)\n",
    "        \n",
    "        print(len(y), \"outliers removed for\", feature)\n",
    "        \n",
    "        df = df.loc[np.abs(df[feature + '_zscore']) < 3]\n",
    "        \n",
    "        df = df.drop([feature + '_zscore'], axis=1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "    \n",
    "def validate_changes(df):\n",
    "    '''Loads the requested df, and provides its shape and count of null values; should be run after initial\n",
    "        transformation\n",
    "    \n",
    "        @params\n",
    "        df is a pd.DataFrame\n",
    "        \n",
    "        @output\n",
    "        summary data\n",
    "    \n",
    "    '''\n",
    "    print('DATA FRAME SHAPE')\n",
    "    print(\"========================================\")\n",
    "    print(df.shape)\n",
    "    print(\"\")\n",
    "    print(\"COLUMNS THAT STILL HAVE NULL VALUES AFTER TRANSFORMATION\")\n",
    "    print(\"(Should Produce No Results Below Line)\")\n",
    "    print(\"========================================\")      \n",
    "    null_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            null_cols.append(col)\n",
    "    for col in null_cols:\n",
    "        print(col, \"VALUE COUNTS\")\n",
    "        print(\"--------------------\")\n",
    "        print(df[col].value_counts(dropna=False))  \n",
    "        print(\"\")\n",
    "    print(\"\")\n",
    "    return df.head()\n",
    "\n",
    "\n",
    "def validate_reg_assumptions(X, X_train, X_test, y_train, y_test):\n",
    "    results = reg_summary(X_train, y_train)\n",
    "    print(results)\n",
    "    \n",
    "    print('\\nIdentifying Residuals...\\n')\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    y_pred = results.predict(X_test)\n",
    "    residual = y_test - y_pred\n",
    "\n",
    "    print('\\nVerifying Normality of Residuals...\\n')\n",
    "    sns.distplot(residual)\n",
    "    plt.show();\n",
    "    draw_qqplot(residual)\n",
    "    plt.show();\n",
    "    print('Mean of Residuals: ', np.mean(residual))\n",
    "\n",
    "    print('\\nDisplaying Regplot...\\n')\n",
    "    draw_scatter(y_pred, residual)\n",
    "    sns.regplot(y_pred, residual, color='red')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
